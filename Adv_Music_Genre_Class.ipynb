{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from tensorflow.keras.utils import to_categorical\nimport torch\nfrom torch.utils.data import Dataset\nimport torchaudio\nfrom scipy.signal import butter, filtfilt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch.nn as nn\nimport os\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nimport tensorflow as tf\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import accuracy_score\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch\nimport torch.nn as nn\nfrom collections import deque\nimport random\n\n\nclass GTZANDataset(Dataset):\n    # Your __init__ method remains the same\n    def __init__(self, directory, exclude_file=None, max_len=675808, mfcc_extraction=False, frame_length=128, frame_step=1024, num_mel_bins=128, n_mfcc=10, low_pass_cutoff=2000, filter_order=5, fft_length=1024, downsample=False):\n        self.directory = directory\n        self.classes = os.listdir(directory)\n        self.class_indices = {cls_name: i for i, cls_name in enumerate(self.classes)}\n        self.samples = []\n        self.targets = []\n        \n        # parameters\n        self.max_len = max_len\n        self.downsample = downsample\n        self.mfcc_extraction = mfcc_extraction\n        self.low_pass_cutoff = low_pass_cutoff\n        self.filter_order = filter_order\n        self.frame_length = frame_length\n        self.frame_step = frame_step\n        self.num_mel_bins = num_mel_bins\n        self.n_mfcc = n_mfcc\n        self.fft_length = fft_length\n        \n\n        # create list of files and list of targets\n        for cls_idx, cls_name in enumerate(self.classes):\n            cls_path = os.path.join(directory, cls_name)\n            filenames = os.listdir(cls_path)\n            for filename in filenames:\n                if filename != exclude_file:\n                    sample_path = os.path.join(cls_path, filename)\n                    self.samples.append(sample_path)\n                    self.targets.append(cls_idx)\n        self.targets = self.one_hot_encode(self.targets, num_classes=len(self.classes))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        sample_path = self.samples[index]\n        target = self.targets[index]\n        processed_waveform, target = self._load_and_process_waveform(sample_path, target)\n        return processed_waveform, target\n\n    def one_hot_encode(self, x, num_classes):\n        return torch.eye(num_classes)[x]\n\n    def _load_and_process_waveform(self, sample_path, target):#\n        chunk_targets = []\n        waveform, sample_rate = torchaudio.load(sample_path)\n        desired_length = self.max_len\n\n        waveform = waveform.squeeze(0)\n        \n        # adjust waveform to desired length\n        if len(waveform) < desired_length * 30:\n            num_zeros = desired_length*30 - len(waveform)\n            pad_left = num_zeros // 2\n            pad_right = num_zeros - pad_left\n            waveform = np.pad(waveform.numpy(), (pad_left, pad_right), mode='constant')            \n        elif len(waveform) > desired_length * 30:\n            start = (len(waveform) - desired_length * 30) // 2\n            end = start + desired_length * 30\n            waveform = waveform[start:end]\n\n            \n        # Split waveform into 1-second chunks\n        chunk_size = sample_rate * 1\n        padding_size = chunk_size - (len(waveform) % chunk_size)\n        waveform = np.pad(waveform, (0, padding_size), mode='constant')\n        waveform = torch.from_numpy(waveform)\n        num_chunks = len(waveform) // chunk_size\n        waveform_chunks = torch.split(waveform[:chunk_size*num_chunks], chunk_size)\n        if waveform.shape[0] % chunk_size != 0:\n            num_zeros = chunk_size - (waveform.shape[0] % chunk_size)\n            waveform = F.pad(waveform, (0, num_zeros), 'constant', 0)\n        processed_chunks = []\n        \n        for chunk in waveform_chunks:            \n            if self.downsample:\n                # downsample\n                def downsample_waveform(waveform, sample_rate):\n                    b, a = butter(self.filter_order, self.low_pass_cutoff / (sample_rate / 2), 'lowpass')\n                    waveform_np = waveform\n                    filtered_waveform_np = filtfilt(b, a, waveform_np.squeeze())\n                    return torch.from_numpy(np.float32(filtered_waveform_np).reshape(1, -1))\n                chunk = downsample_waveform(chunk, sample_rate)\n                chunk = chunk.squeeze(0)\n            \n            \n            if self.mfcc_extraction:\n                # mfcc extraction\n                mfcc_transform = torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=self.n_mfcc, melkwargs={'n_fft': self.fft_length, 'n_mels': self.num_mel_bins, 'hop_length': self.frame_step, 'win_length': self.frame_length})\n\n                mfccs = mfcc_transform(chunk)\n                processed_chunks.append(mfccs)\n                chunk_targets.append(target)\n            else:\n                processed_chunks.append(chunk)\n                chunk_targets.append(target)\n                \n        return processed_chunks, chunk_targets\n ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-25T14:23:23.891181Z","iopub.execute_input":"2023-04-25T14:23:23.891636Z","iopub.status.idle":"2023-04-25T14:23:24.590399Z","shell.execute_reply.started":"2023-04-25T14:23:23.891598Z","shell.execute_reply":"2023-04-25T14:23:24.588985Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class LSTMNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n        super(LSTMNet, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.bn = nn.BatchNorm1d(hidden_size)\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_size, 32),\n            nn.BatchNorm1d(32),\n            nn.Dropout(dropout),\n            nn.Linear(32, num_classes),           \n        )\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        batch_size = np.shape(x)[0]\n        \n        out, _ = self.lstm(x, (h0, c0))\n        out = out[:, -1, :]\n        out = self.bn(out)\n        out = self.dropout(out)\n        out = self.fc(out)\n        return out\n\n    \n\n# Parameters\nhidden_size = 32\nnum_layers = 2\nnum_epochs = 100\nn_mfcc = 35\nmax_len=22050\nmfcc_extraction=True\nframe_length = 256\nframe_step= 200\nnum_mel_bins=128\nlow_pass_cutoff=2000\nfilter_order=5\ndownsample=True\nfft_length = 1024\nbatch_size = 32\n\nnum_classes = 10  \ninput_size = n_mfcc  \n\ndirectory = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original\"\nexclude_file = \"jazz.00054.wav\"\ndataset = GTZANDataset(directory, exclude_file=exclude_file, max_len=max_len, mfcc_extraction=mfcc_extraction, frame_length=frame_length, frame_step=frame_step, num_mel_bins=num_mel_bins, n_mfcc=n_mfcc, low_pass_cutoff=low_pass_cutoff, filter_order=filter_order, fft_length=fft_length, downsample=downsample)\n\n# normalize dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((-1, -1, -1), (2, 2, 2)) \n])\n\n\ndataset.transform = transform\n\n# Instantiate the model\nmodel = LSTMNet(input_size, hidden_size, num_layers, num_classes)\n\n# Define the ratios for train, validation, and test\ntrain_ratio = 0.7\nval_ratio = 0.2\ntest_ratio = 0.1\n\n# Calculate the lengths of each split\ntrain_len = int(train_ratio * len(dataset))\nval_len = int(val_ratio * len(dataset))\ntest_len = len(dataset) - train_len - val_len\n\n\n\n# Use random_split to split the dataset\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n\n# define dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-04-25T14:23:27.438654Z","iopub.execute_input":"2023-04-25T14:23:27.439083Z","iopub.status.idle":"2023-04-25T14:23:27.816990Z","shell.execute_reply.started":"2023-04-25T14:23:27.439048Z","shell.execute_reply":"2023-04-25T14:23:27.816098Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train(model, train_loader, criterion, optimizer, device):\n    model.to(device)\n    train_loss, train_total, train_correct = 0.0, 0, 0\n    model.train()\n    counter = 0\n    for inputchunks, labelchunks in train_loader:\n\n        for x in range(len(inputchunks)):\n            inputs, labels = inputchunks[x].to(device), labelchunks[x].to(device)\n            optimizer.zero_grad()\n                            \n            inputs = inputs.permute(0,2,1)\n            outputs = model(inputs)\n            predicted = outputs.data.argmax(dim=1)\n            \n            # train model\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            # calculate loss and acc\n            train_loss += loss.item()\n            counter += (inputs.size(0))\n            train_total += labels.size(0)\n            _, labels = labels.max(dim=1)\n            train_correct += (predicted == labels).sum().item()\n            \n    train_loss /= (len(train_loader.dataset) *  len(inputchunks))\n    train_acc = train_correct / (train_total)\n\n          \n    return train_loss, train_acc","metadata":{"execution":{"iopub.status.busy":"2023-04-24T12:24:22.574720Z","iopub.execute_input":"2023-04-24T12:24:22.575078Z","iopub.status.idle":"2023-04-24T12:24:22.583954Z","shell.execute_reply.started":"2023-04-24T12:24:22.575044Z","shell.execute_reply":"2023-04-24T12:24:22.582789Z"},"trusted":true},"execution_count":212,"outputs":[]},{"cell_type":"code","source":"#for validation and test datasets\ndef validate(model, dataloader, criterion, device):\n    \n    model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for inputchunks, labelchunks in dataloader:\n            for x in range(len(inputchunks)):\n                \n                inputs = inputchunks[x].to(device)\n                labels = labelchunks[x].to(device)\n                #if downsample:\n                #    inputs = np.squeeze(inputs, axis=1)\n                \n                inputs = inputs.permute(0,2,1)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                _, labels = labels.max(dim=1)\n                \n                # calculate loss and acc\n                val_loss += loss.item()\n                predicted = outputs.data.argmax(dim=1)\n                val_correct += (predicted == labels).sum().item()\n                val_total += labels.size(0)\n                \n    val_loss /= (len(dataloader.dataset) * len(inputchunks))\n    val_acc = val_correct / (val_total)\n    \n    return val_loss, val_acc","metadata":{"execution":{"iopub.status.busy":"2023-04-24T12:24:22.586494Z","iopub.execute_input":"2023-04-24T12:24:22.587971Z","iopub.status.idle":"2023-04-24T12:24:22.599852Z","shell.execute_reply.started":"2023-04-24T12:24:22.587927Z","shell.execute_reply":"2023-04-24T12:24:22.598840Z"},"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"code","source":"\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.0001\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4) # Set weight decay here\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n\n\nbest_val_loss = 1\n\n# Define variables to store training and validation losses and accuracies\ntrain_losses = []\ntrain_accs = []\nval_losses = []\nval_accs = []\npatience = 5\npatience_counter = 0\nfor epoch in range(num_epochs):\n    # Train the model\n    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n\n    # Validate the model\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n    # Save the model if the validation loss has decreased\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'lstm_net.pth')\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    # Early stopping\n    if patience_counter >= patience:\n        print('Validation stopped improving for ', patience, 'epochs. Stopping early.')\n        \n    scheduler.step(val_loss)\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    \n    # Print the training and validation loss and accuracy for this epoch\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\ntest_loss, test_acc = validate(model, test_loader, criterion, device)\n\nprint(\"Test Acc: \", test_acc, \"Test loss: \", test_loss)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T12:24:22.601621Z","iopub.execute_input":"2023-04-24T12:24:22.602158Z","iopub.status.idle":"2023-04-24T13:15:09.233956Z","shell.execute_reply.started":"2023-04-24T12:24:22.602121Z","shell.execute_reply":"2023-04-24T13:15:09.231373Z"},"trusted":true},"execution_count":214,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 2.2432, Train Acc: 0.1797, Val Loss: 2.1315, Val Acc: 0.2866\nEpoch 2/100, Train Loss: 2.0429, Train Acc: 0.2496, Val Loss: 2.0860, Val Acc: 0.3101\nEpoch 3/100, Train Loss: 1.9316, Train Acc: 0.2910, Val Loss: 2.0175, Val Acc: 0.3255\nEpoch 4/100, Train Loss: 1.8786, Train Acc: 0.3044, Val Loss: 2.0428, Val Acc: 0.3291\nEpoch 5/100, Train Loss: 1.8249, Train Acc: 0.3322, Val Loss: 1.9441, Val Acc: 0.3458\nEpoch 6/100, Train Loss: 1.7858, Train Acc: 0.3454, Val Loss: 1.9542, Val Acc: 0.3633\nEpoch 7/100, Train Loss: 1.7285, Train Acc: 0.3689, Val Loss: 1.7895, Val Acc: 0.3620\nEpoch 8/100, Train Loss: 1.6899, Train Acc: 0.3846, Val Loss: 1.9916, Val Acc: 0.3561\nEpoch 9/100, Train Loss: 1.6746, Train Acc: 0.3942, Val Loss: 1.9972, Val Acc: 0.3809\nEpoch 10/100, Train Loss: 1.6353, Train Acc: 0.4070, Val Loss: 1.8649, Val Acc: 0.3646\nEpoch 11/100, Train Loss: 1.5994, Train Acc: 0.4210, Val Loss: 1.8670, Val Acc: 0.3965\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 12/100, Train Loss: 1.5764, Train Acc: 0.4284, Val Loss: 1.8258, Val Acc: 0.3946\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 00013: reducing learning rate of group 0 to 3.0000e-05.\nEpoch 13/100, Train Loss: 1.5532, Train Acc: 0.4448, Val Loss: 1.8897, Val Acc: 0.3869\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 14/100, Train Loss: 1.5404, Train Acc: 0.4472, Val Loss: 1.8559, Val Acc: 0.4129\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 15/100, Train Loss: 1.5362, Train Acc: 0.4450, Val Loss: 1.9290, Val Acc: 0.4273\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 16/100, Train Loss: 1.5149, Train Acc: 0.4515, Val Loss: 1.9459, Val Acc: 0.4101\nEpoch 17/100, Train Loss: 1.5145, Train Acc: 0.4562, Val Loss: 1.7216, Val Acc: 0.4237\nEpoch 18/100, Train Loss: 1.5105, Train Acc: 0.4501, Val Loss: 1.7204, Val Acc: 0.4312\nEpoch 19/100, Train Loss: 1.5157, Train Acc: 0.4519, Val Loss: 1.7822, Val Acc: 0.4333\nEpoch 20/100, Train Loss: 1.4886, Train Acc: 0.4636, Val Loss: 1.8238, Val Acc: 0.4357\nEpoch 21/100, Train Loss: 1.4960, Train Acc: 0.4605, Val Loss: 1.7631, Val Acc: 0.4362\nEpoch 22/100, Train Loss: 1.4789, Train Acc: 0.4658, Val Loss: 1.8498, Val Acc: 0.4276\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 23/100, Train Loss: 1.4759, Train Acc: 0.4664, Val Loss: 1.8138, Val Acc: 0.4297\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 00024: reducing learning rate of group 0 to 9.0000e-06.\nEpoch 24/100, Train Loss: 1.4758, Train Acc: 0.4661, Val Loss: 1.8850, Val Acc: 0.4386\nEpoch 25/100, Train Loss: 1.4589, Train Acc: 0.4750, Val Loss: 1.7146, Val Acc: 0.4231\nEpoch 26/100, Train Loss: 1.4697, Train Acc: 0.4673, Val Loss: 1.7829, Val Acc: 0.4242\nEpoch 27/100, Train Loss: 1.4643, Train Acc: 0.4702, Val Loss: 1.7523, Val Acc: 0.4456\nEpoch 28/100, Train Loss: 1.4675, Train Acc: 0.4682, Val Loss: 1.6943, Val Acc: 0.4373\nEpoch 29/100, Train Loss: 1.4701, Train Acc: 0.4716, Val Loss: 1.7577, Val Acc: 0.4411\nEpoch 30/100, Train Loss: 1.4722, Train Acc: 0.4700, Val Loss: 1.8552, Val Acc: 0.4276\nEpoch 31/100, Train Loss: 1.4580, Train Acc: 0.4787, Val Loss: 1.8473, Val Acc: 0.4361\nEpoch 32/100, Train Loss: 1.4639, Train Acc: 0.4677, Val Loss: 1.8122, Val Acc: 0.4292\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 33/100, Train Loss: 1.4634, Train Acc: 0.4710, Val Loss: 1.8747, Val Acc: 0.4545\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 00034: reducing learning rate of group 0 to 2.7000e-06.\nEpoch 34/100, Train Loss: 1.4549, Train Acc: 0.4757, Val Loss: 1.8000, Val Acc: 0.4401\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 35/100, Train Loss: 1.4609, Train Acc: 0.4694, Val Loss: 1.7247, Val Acc: 0.4446\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 36/100, Train Loss: 1.4472, Train Acc: 0.4768, Val Loss: 1.7394, Val Acc: 0.4333\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 37/100, Train Loss: 1.4648, Train Acc: 0.4697, Val Loss: 1.8119, Val Acc: 0.4352\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 38/100, Train Loss: 1.4536, Train Acc: 0.4781, Val Loss: 1.7695, Val Acc: 0.4317\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 39/100, Train Loss: 1.4490, Train Acc: 0.4783, Val Loss: 1.7908, Val Acc: 0.4346\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 00040: reducing learning rate of group 0 to 8.1000e-07.\nEpoch 40/100, Train Loss: 1.4654, Train Acc: 0.4715, Val Loss: 1.7620, Val Acc: 0.4461\nEpoch 41/100, Train Loss: 1.4553, Train Acc: 0.4751, Val Loss: 1.6909, Val Acc: 0.4516\nEpoch 42/100, Train Loss: 1.4517, Train Acc: 0.4771, Val Loss: 1.8569, Val Acc: 0.4390\nEpoch 43/100, Train Loss: 1.4504, Train Acc: 0.4806, Val Loss: 1.7689, Val Acc: 0.4351\nEpoch 44/100, Train Loss: 1.4528, Train Acc: 0.4803, Val Loss: 1.7298, Val Acc: 0.4456\nEpoch 45/100, Train Loss: 1.4464, Train Acc: 0.4799, Val Loss: 1.7437, Val Acc: 0.4382\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 46/100, Train Loss: 1.4547, Train Acc: 0.4733, Val Loss: 1.7336, Val Acc: 0.4380\nValidation loss has not improved for the past 5 epochs. Stopping early.\nEpoch 00047: reducing learning rate of group 0 to 2.4300e-07.\nEpoch 47/100, Train Loss: 1.4264, Train Acc: 0.4849, Val Loss: 1.7897, Val Acc: 0.4466\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3016778875.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1852772770.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelchunks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1373756298.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0msample_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mprocessed_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_and_process_waveform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1373756298.py\u001b[0m in \u001b[0;36m_load_and_process_waveform\u001b[0;34m(self, sample_path, target)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_and_process_waveform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mchunk_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mdesired_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     ret = torch.ops.torchaudio.sox_io_load_audio_file(\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def phase_shuffle(x, shuffle_factor = 2):\n\n    # Calculate required padding for shuffling\n    pad_left = shuffle_factor // 2\n    pad_right = shuffle_factor - pad_left\n\n    # Randomly select a value from the range [-pad_left, pad_right] for each sample in the batch\n    shuffle_offsets = torch.randint(-pad_left, pad_right + 1, (x.size(0), 1, 1)).to(x.device)\n\n    # Pad the input tensor before shuffling\n    x_padded = F.pad(x, (pad_left, pad_right), mode='reflect')\n\n    # Create a list to store the shuffled tensors\n    shuffled_tensors = []\n\n    # Loop through each sample in the batch\n    for i, shuffle_offset in enumerate(shuffle_offsets.squeeze()):\n        \n        # Slice the tensor according to the shuffle offset\n        start = pad_left + shuffle_offset.item()\n        end = x_padded.size(2) - pad_right + shuffle_offset.item()\n        shuffled_tensors.append(x_padded[i, :, start:end])\n\n    # Stack the shuffled tensors back into a single tensor\n    x_shuffled = torch.stack(shuffled_tensors, dim=0)\n\n    return x_shuffled\n\n#MFCC WaveGAN generator implementation\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        # Fully connected layer\n        self.fc = nn.Sequential(\n            nn.Linear(110,1400),\n            nn.BatchNorm1d(1400),\n            nn.LeakyReLU(),\n            \n        )\n        \n        # convolutional layers\n        self.transconv_layers = nn.Sequential(\n            nn.ConvTranspose1d(140, 70, kernel_size=25, stride=4, padding=11, output_padding=0),\n            nn.BatchNorm1d(70),\n            nn.LeakyReLU(),\n            nn.Dropout(0.3),\n            nn.ConvTranspose1d(70, 35, kernel_size=25, stride=4, padding=11, output_padding=0),\n            nn.BatchNorm1d(35),\n            nn.LeakyReLU(),\n            nn.Dropout(0.3),\n\n            nn.ConvTranspose1d(35, 35, kernel_size=25, stride=4, padding=11, output_padding=0),\n            nn.BatchNorm1d(35),\n            nn.LeakyReLU(),\n            nn.Dropout(0.3),\n\n            nn.ConvTranspose1d(35, 35, kernel_size=25, stride=4, padding=11, output_padding=0),\n            nn.Tanh()\n        )\n\n    def forward(self, mfcc, genre_label):\n        x = torch.cat((mfcc.view(mfcc.size(0), -1), genre_label), dim=1)\n        x = self.fc(x)\n        \n        x = x.view(x.size(0), 140, 10)\n        \n        x = self.transconv_layers(x)\n        x = x[:,:,:111]\n        return x\n\n#MFCC WaveGAN descriminator implementation\nclass Discriminator(nn.Module):\n    def __init__(self, shuffle_range=2):\n        super(Discriminator, self).__init__()\n        \n        self.conv1 = nn.Conv1d(35, 16, kernel_size=25, stride=4, padding=11)\n        self.lr =  nn.LeakyReLU(0.2)\n        self.conv2 =   nn.Conv1d(16, 32, kernel_size=25, stride=4, padding=11)\n        self.conv3 =    nn.Conv1d(32, 64, kernel_size=25, stride=4, padding=11)\n        self.conv4 =    nn.Conv1d(64, 128, kernel_size=20, stride=4, padding=11)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(266, 1),\n            nn.Sigmoid()\n        )\n        \n        self.bn1 = nn.BatchNorm1d(16)\n        self.bn2 = nn.BatchNorm1d(32)\n        self.bn3 = nn.BatchNorm1d(64)\n        self.bn4 = nn.BatchNorm1d(128)\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x, genre_label):        \n        x = self.conv1(x)\n        self.lr(x)\n        phase_shuffle(x)\n        self.dropout = nn.Dropout(0.3)\n        x = self.bn1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        self.lr(x)\n\n        phase_shuffle(x)\n        self.dropout = nn.Dropout(0.5)\n\n        x = self.conv3(x)\n\n        self.lr(x)\n        phase_shuffle(x)\n        self.dropout = nn.Dropout(0.5)\n        x = self.bn3(x)\n\n        x = self.conv4(x)\n\n        self.lr(x)\n        phase_shuffle(x)\n        self.dropout = nn.Dropout(0.5)\n        x = self.bn4(x)\n  \n        x = x.view(x.size(0), -1)\n                \n        x = torch.cat((x, genre_label), dim=1)\n        x = self.fc(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T10:19:26.919513Z","iopub.execute_input":"2023-04-25T10:19:26.920003Z","iopub.status.idle":"2023-04-25T10:19:26.948631Z","shell.execute_reply.started":"2023-04-25T10:19:26.919961Z","shell.execute_reply":"2023-04-25T10:19:26.947232Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained LSTM model\ntimesteps = 111\nnum_classes = 10\npre_trained_model = LSTMNet(input_size, hidden_size, num_layers, num_classes)\n#pre_trained_model.load_state_dict(torch.load('/kaggle/input/musicgenrelstm/lstm_net.pth'))\npre_trained_model.load_state_dict(torch.load('/kaggle/working/lstm_net.pth'))\n\nfor param in pre_trained_model.parameters():\n    param.requires_grad = False\n\n# Instantiate new discriminator with transfer learning\nz_dim = 100\nn_classes = 10\nD = Discriminator()\nG = Generator()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T10:19:29.326168Z","iopub.execute_input":"2023-04-25T10:19:29.327446Z","iopub.status.idle":"2023-04-25T10:19:29.732248Z","shell.execute_reply.started":"2023-04-25T10:19:29.327392Z","shell.execute_reply":"2023-04-25T10:19:29.730736Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/104045666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpre_trained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#pre_trained_model.load_state_dict(torch.load('/kaggle/input/musicgenrelstm/lstm_net.pth'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpre_trained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/lstm_net.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpre_trained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/lstm_net.pth'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/lstm_net.pth'","output_type":"error"}]},{"cell_type":"code","source":"from collections import deque\nimport random\n\ndef train_gan(G, D, train_dataloader,val_dataloader, num_epochs, z_dim, n_classes, n_mfcc, device):\n    # Define loss function\n    criterion = nn.BCELoss()\n    counter = 0\n    othercounter = 0\n    # Define optimizers\n    optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0007, weight_decay = 0.0001, betas=(0.3, 0.999))\n    optimizer_D = torch.optim.Adam(D.parameters(), lr=0.000,weight_decay = 0.0001, betas=(0.3, 0.999))\n    # Initialize Replay Buffer\n    replay_buffer = ReplayBuffer(128)\n    \n    # Move models to device\n\n    # Set pre-trained model to evaluation mode\n    G.to(device)\n    D.to(device)\n\n    for epoch in range(num_epochs):\n        g_correct = 0\n        g_total = 0\n        g_val_correct = 0\n        g_val_total = 0\n        d_total = 0\n        d_correct = 0\n        d_val_correct = 0\n        d_val_total = 0\n        G.train()\n        D.train()\n        train_G_loss = 0\n        train_D_loss = 0\n        for inputchunks, labelchunks in train_dataloader:\n            othercounter += 1\n            for x in range(len(inputchunks)):\n\n                # Move data to device\n                optimizer_D.zero_grad()\n                mfccs =  inputchunks[x].to(device)              \n                labels = labelchunks[x].to(device)\n                batch_size = mfccs.size(0)\n                \n                # Real data\n                real_labels = torch.ones(batch_size, 1).uniform_(0.7, 1.0).to(device)  # Assuming device is set up\n                real_data = mfccs\n                real_output = D(real_data, labels)\n                real_loss = criterion(real_output, real_labels)\n                real_loss.backward()\n                optimizer_D.step()\n                optimizer_D.zero_grad()\n                \n                # Fake data\n                noise = torch.randn(batch_size, z_dim).to(device)  # Assuming z_dim is the noise dimension\n                fake_data = G(noise, labels)\n                fake_labels = torch.zeros(batch_size, 1).uniform_(0.0, 0.3).to(device)\n                fake_output = D(fake_data.detach(), labels)\n                fake_loss = criterion(fake_output, fake_labels)\n               \n                \n                # Update discriminator\n                fake_loss.backward()\n                optimizer_D.step()\n                d_loss = (real_loss.item() + fake_loss.item())/2\n                train_D_loss += d_loss\n\n                # calculate accuracy\n                d_total += real_labels.size(0)\n                d_correct += (torch.round(real_output) == torch.round(real_labels)).sum().item()\n                d_total += labels.size(0)\n                d_correct += (torch.round(fake_output) == torch.round(fake_labels)).sum().item()\n                \n                # Train the generator\n                optimizer_G.zero_grad()\n\n                # Flip the labels for the generator\n                flipped_labels = torch.ones(batch_size, 1).uniform_(0.7, 1.0).to(device)\n                output1 = D(fake_data, labels)\n                g_loss1 = criterion(output1, flipped_labels)\n                g_correct += (torch.round(output1) == torch.round(flipped_labels)).sum().item()\n                g_total += flipped_labels.size(0)\n                \n                # Update generator\n                g_loss1.backward()     \n                optimizer_G.step()\n                optimizer_G.zero_grad()\n                \n                # Fake data\n                noise = torch.randn(batch_size, z_dim).to(device)  # Assuming z_dim is the noise dimension\n                fake_data = G(noise, labels)\n                \n                # Flip the labels for the generator\n                flipped_labels = torch.ones(batch_size, 1).uniform_(0.7, 1.0).to(device)\n                output2 = D(fake_data, labels)\n                g_loss2 = criterion(output2, flipped_labels)\n                g_correct += (torch.round(output2) == torch.round(flipped_labels)).sum().item()\n                g_total += flipped_labels.size(0)\n                train_G_loss += ((g_loss1.item() + g_loss2.item())/2)\n\n                # Update generator\n                g_loss2.backward()     \n                optimizer_G.step()\n                \n                counter += 1\n                                \n        g_grad_norm = 0.0        \n        d_grad_norm = 0.0\n        \n        # calculate grad norms to ensure functional training\n        for p in D.parameters():\n            if p.grad is not None:\n                d_grad_norm += p.grad.norm(2).item()  # Use L2 norm\n        d_grad_norm = np.sqrt(d_grad_norm)\n        print(\"Discriminator gradient norm:\", d_grad_norm)\n        for p in G.parameters():\n            if p.grad is not None:\n                g_grad_norm += p.grad.norm(2).item()  # Use L2 norm\n        g_grad_norm = np.sqrt(g_grad_norm)\n        print(\"Generator gradient norm:\", g_grad_norm)\n    \n        train_G_loss /= len(train_dataset) \n        train_D_loss /= len(train_dataset) \n        G.eval()\n        D.eval()\n        val_loss = 0\n        val_acc = 0\n        num_batches = 0\n        g_val_loss = 0\n        d_val_loss = 0\n        if epoch % 2 == 0:\n            \n            # Validation\n            with torch.no_grad():\n                for inputchunks, labelchunks in val_dataloader:\n                    for x in range(len(inputchunks)):\n\n                        inputs =  inputchunks[x].to(device)\n                        labels = labelchunks[x].to(device)\n                        \n                        batch_size = inputs.size(0)\n\n                        # Real data\n                        real_labels = torch.ones(batch_size, 1).uniform_(0.7, 1.0).to(device)\n                        real_data = inputs\n                        real_output = D(real_data, labels)\n                        real_loss = criterion(real_output, real_labels)\n\n                        # Fake data\n                        noise = torch.randn(batch_size, z_dim).to(device)\n                        fake_data = G(noise, labels)\n                        fake_labels = torch.zeros(batch_size, 1).uniform_(0.0, 0.3).to(device)\n                        fake_output = D(fake_data.detach(), labels)\n                        fake_loss = criterion(fake_output, fake_labels)\n\n                        # discriminator accuracy and loss\n                        d_val_total += real_labels.size(0)\n                        d_val_correct += (torch.round(real_output) == torch.round(real_labels)).sum().item()\n                        d_val_total += labels.size(0)\n                        d_val_correct += (torch.round(fake_output) == torch.round(fake_labels)).sum().item()\n                        d_loss = (real_loss + fake_loss)/2\n\n                        # Generator loss\n                        flipped_labels = torch.ones(batch_size, 1).uniform_(0.7, 1.0).to(device)\n                        output = D(fake_data, labels)\n                        g_loss = criterion(output, flipped_labels)\n                        \n                        # generator accuracy\n                        g_val_correct += (torch.round(output) == torch.round(flipped_labels)).sum().item()\n                        g_val_total += flipped_labels.size(0)\n                        g_val_loss += g_loss.item()\n                        d_val_loss += d_loss\n                        num_batches += 1\n\n            g_val_loss /= len(val_dataset)\n            d_val_loss /= len(val_dataset)\n            g_acc = ( g_correct / g_total)\n            g_val_acc = g_val_correct / g_val_total\n            d_acc = d_correct / d_total         \n            d_val_acc = d_val_correct / d_val_total\n            \n            print(\"[Epoch %d/%d] [G loss %f] [D loss %f] [G V loss: %f] [D Val loss: %f]\"\n                      % (epoch, num_epochs,train_G_loss, train_D_loss, g_val_loss, d_val_loss))\n            print(\"[Epoch %d/%d] [G Acc %f] [D Acc %f] [G V Acc: %f] [D Val Acc: %f]\"\n                      % (epoch, num_epochs,g_acc, d_acc, g_val_acc, d_val_acc))\n            \n    return generator, discriminator\n","metadata":{"execution":{"iopub.status.busy":"2023-04-24T22:43:36.940582Z","iopub.execute_input":"2023-04-24T22:43:36.940980Z","iopub.status.idle":"2023-04-24T22:43:36.971692Z","shell.execute_reply.started":"2023-04-24T22:43:36.940945Z","shell.execute_reply":"2023-04-24T22:43:36.970326Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"lstm = LSTMNet(input_size, hidden_size, num_layers, num_classes)\nlstm.load_state_dict(torch.load('/kaggle/input/finalgan/lstm_net (2).pth'))\n#for param in lstm.parameters():\n#    param.requires_grad = False\n    \nG = Generator().to(device)\nG.load_state_dict(torch.load('/kaggle/input/finalgan/G.pth'))\nfor param in G.parameters():\n    param.requires_grad = False\n    \nD = Discriminator().to(device)\nD.load_state_dict(torch.load('/kaggle/input/finalgan/D.pth'))\nfor param in D.parameters():\n    param.requires_grad = False\n    \ng_test_loss = 0\nd_test_loss = 0\nd_total = 0\ng_total = 0\nd_correct = 0\ng_correct = 0\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncriterion = nn.BCELoss()\nz_dim= 100\n\nwith torch.no_grad():\n    for inputchunks, labelchunks in test_loader:\n\n        for x in range(len(inputchunks)):\n\n            inputs =  inputchunks[x].to(device)\n            labels = labelchunks[x].to(device)\n\n            batch_size = inputs.size(0)\n\n            # Real data\n            real_labels = torch.ones(batch_size, 1).uniform_(0.7, 1.0).to(device)\n            real_data = inputs\n            real_output = D(real_data, labels)\n            real_loss = criterion(real_output, real_labels)\n\n            # Fake data\n            noise = torch.randn(batch_size, z_dim).to(device)\n            fake_data = G(noise, labels)\n            fake_labels = torch.zeros(batch_size, 1).uniform_(0.0, 0.3).to(device)\n            fake_output = D(fake_data.detach(), labels)\n            \n            # Calculate Discriminator loss and accuracy\n            fake_loss = criterion(fake_output, fake_labels)\n            d_total += real_labels.size(0)\n            d_correct += (torch.round(real_output) == torch.round(real_labels)).sum().item()\n            d_total += labels.size(0)\n            d_correct += (torch.round(fake_output) == torch.round(fake_labels)).sum().item()\n            d_loss = (real_loss + fake_loss)/2\n\n            flipped_labels = torch.ones(batch_size, 1).uniform_(0.7, 1.0).to(device)\n            output = D(fake_data, labels)\n            \n            # calculate generator loss and accuracy\n            g_loss = criterion(output, flipped_labels) \n            g_correct += (torch.round(output) == torch.round(flipped_labels)).sum().item()\n            g_total += flipped_labels.size(0)\n            g_test_loss += g_loss.item()\n            d_test_loss += d_loss.item()\n\ng_test_loss /= g_total\nd_test_loss /= d_total\nd_test_acc = d_correct/d_total\ng_test_acc = g_correct / g_total\n\nprint(\"G Loss: \", g_test_loss, \"G Acc: \", g_test_acc, \"D Acc: \", d_test_acc, \"D Loss: \", d_test_loss)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.0001\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4) # Set weight decay here\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T10:19:53.516611Z","iopub.execute_input":"2023-04-25T10:19:53.517830Z","iopub.status.idle":"2023-04-25T10:19:53.582824Z","shell.execute_reply.started":"2023-04-25T10:19:53.517785Z","shell.execute_reply":"2023-04-25T10:19:53.581017Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3779703563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/finalgan/lstm_net (2).pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         loaded_storages[key] = torch.storage.TypedStorage(\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m             dtype=dtype)\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    167\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."],"ename":"RuntimeError","evalue":"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.","output_type":"error"}]},{"cell_type":"code","source":"eps = 1e-8\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntorch.save(G.state_dict(), 'G.pth')\ntorch.save(D.state_dict(), 'D.pth')\n\ntrain_gan(G, D, pre_trained_model, train_loader, val_loader, 100, z_dim, num_classes,n_mfcc, device)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T17:57:06.626875Z","iopub.execute_input":"2023-04-24T17:57:06.627179Z","iopub.status.idle":"2023-04-24T19:01:38.365774Z","shell.execute_reply.started":"2023-04-24T17:57:06.627149Z","shell.execute_reply":"2023-04-24T19:01:38.363852Z"},"trusted":true},"execution_count":419,"outputs":[{"name":"stdout","text":"Discriminator gradient norm: 7.182796636097675\nGenerator gradient norm: 4.571407222212066\n[Epoch 0/100] [G loss 1.566662] [D loss 0.493915] [G V loss: 1.165299] [D Val Loss: 0.800600]\nDiscriminator gradient norm: 6.653870177186063\nGenerator gradient norm: 3.2879697611088776\nDiscriminator gradient norm: 7.830081908503177\nGenerator gradient norm: 5.454021837677077\n[Epoch 2/100] [G loss 1.409869] [D loss 0.564905] [G V loss: 1.125888] [D Val Loss: 0.989090]\nDiscriminator gradient norm: 6.486670811013982\nGenerator gradient norm: 4.912459322460952\nDiscriminator gradient norm: 5.641413290569193\nGenerator gradient norm: 4.083375017333117\n[Epoch 4/100] [G loss 1.009428] [D loss 0.626718] [G V loss: 0.709896] [D Val Loss: 0.626426]\nDiscriminator gradient norm: 3.661020443371123\nGenerator gradient norm: 2.9931117929871163\nDiscriminator gradient norm: 5.045995891315822\nGenerator gradient norm: 4.008169037808908\n[Epoch 6/100] [G loss 0.785235] [D loss 0.644994] [G V loss: 0.954401] [D Val Loss: 0.622131]\nDiscriminator gradient norm: 2.2654238265224307\nGenerator gradient norm: 1.8438085556474115\nDiscriminator gradient norm: 1.717517744584606\nGenerator gradient norm: 1.2301731303510184\n[Epoch 8/100] [G loss 0.668980] [D loss 0.647832] [G V loss: 0.665807] [D Val Loss: 0.653268]\nDiscriminator gradient norm: 1.5437737312119875\nGenerator gradient norm: 1.1279592273417722\nDiscriminator gradient norm: 1.3749913236023885\nGenerator gradient norm: 0.7839996569616718\n[Epoch 10/100] [G loss 0.664054] [D loss 0.644688] [G V loss: 0.603131] [D Val Loss: 0.753475]\nDiscriminator gradient norm: 1.2054778876058838\nGenerator gradient norm: 0.6733380235791032\nDiscriminator gradient norm: 3.82170282778616\nGenerator gradient norm: 3.065982042241922\n[Epoch 12/100] [G loss 0.853228] [D loss 0.610258] [G V loss: 0.842312] [D Val Loss: 0.721968]\nDiscriminator gradient norm: 1.3415329365302797\nGenerator gradient norm: 0.7219279684480517\nDiscriminator gradient norm: 1.236163281606496\nGenerator gradient norm: 0.5225769358912927\n[Epoch 14/100] [G loss 0.653256] [D loss 0.646901] [G V loss: 0.581302] [D Val Loss: 0.717718]\nDiscriminator gradient norm: 1.5776850980972716\nGenerator gradient norm: 1.062621750909252\nDiscriminator gradient norm: 3.5447737821772822\nGenerator gradient norm: 0.7951347629624448\n[Epoch 16/100] [G loss 0.827777] [D loss 0.598535] [G V loss: 1.112653] [D Val Loss: 0.944483]\nDiscriminator gradient norm: 4.018113659910619\nGenerator gradient norm: 0.8576152195724093\nDiscriminator gradient norm: 4.31365691884369\nGenerator gradient norm: 4.0822877294758\n[Epoch 18/100] [G loss 1.150069] [D loss 0.538418] [G V loss: 0.935308] [D Val Loss: 0.579309]\nDiscriminator gradient norm: 2.182507184268381\nGenerator gradient norm: 2.597487610822263\nDiscriminator gradient norm: 3.2311367072790005\nGenerator gradient norm: 3.90268034605066\n[Epoch 20/100] [G loss 0.812644] [D loss 0.624277] [G V loss: 0.706169] [D Val Loss: 0.629112]\nDiscriminator gradient norm: 1.4112570590091043\nGenerator gradient norm: 0.6338037978108467\nDiscriminator gradient norm: 1.1073017643034442\nGenerator gradient norm: 0.46816637495718444\n[Epoch 22/100] [G loss 0.655344] [D loss 0.644336] [G V loss: 0.634276] [D Val Loss: 0.642537]\nDiscriminator gradient norm: 1.1149974703457282\nGenerator gradient norm: 0.4524696345026208\nDiscriminator gradient norm: 1.0534930172625439\nGenerator gradient norm: 0.36562439386944234\n[Epoch 24/100] [G loss 0.662834] [D loss 0.645125] [G V loss: 0.631536] [D Val Loss: 0.637057]\nDiscriminator gradient norm: 0.8972810445072856\nGenerator gradient norm: 0.3437856200777976\nDiscriminator gradient norm: 1.0750564509852942\nGenerator gradient norm: 0.4507908704471718\n[Epoch 26/100] [G loss 0.646493] [D loss 0.648128] [G V loss: 0.651587] [D Val Loss: 0.665895]\nDiscriminator gradient norm: 3.271380633303016\nGenerator gradient norm: 0.7883907580261218\nDiscriminator gradient norm: 3.112179341906361\nGenerator gradient norm: 0.5246653872859889\n[Epoch 28/100] [G loss 1.439463] [D loss 0.414283] [G V loss: 1.303076] [D Val Loss: 0.564404]\nDiscriminator gradient norm: 2.996915159391067\nGenerator gradient norm: 0.11852657711039544\nDiscriminator gradient norm: 3.4484606983677155\nGenerator gradient norm: 0.9050333330801781\n[Epoch 30/100] [G loss 1.350672] [D loss 0.442720] [G V loss: 1.199437] [D Val Loss: 0.670660]\nDiscriminator gradient norm: 3.4243542102745357\nGenerator gradient norm: 0.9694759120986416\nDiscriminator gradient norm: 3.1422806541320343\nGenerator gradient norm: 0.9137930853678488\n[Epoch 32/100] [G loss 1.423137] [D loss 0.413319] [G V loss: 1.028566] [D Val Loss: 1.460734]\nDiscriminator gradient norm: 3.198370651220614\nGenerator gradient norm: 0.26067649496560835\nDiscriminator gradient norm: 3.0948429399102073\nGenerator gradient norm: 0.6696672912791819\n[Epoch 34/100] [G loss 1.438010] [D loss 0.413312] [G V loss: 1.508490] [D Val Loss: 0.808055]\nDiscriminator gradient norm: 2.9203457831459394\nGenerator gradient norm: 0.20316486228569364\nDiscriminator gradient norm: 3.3564321958997305\nGenerator gradient norm: 1.3183056204891455\n[Epoch 36/100] [G loss 1.255975] [D loss 0.480762] [G V loss: 1.100280] [D Val Loss: 0.834977]\nDiscriminator gradient norm: 4.113932054145252\nGenerator gradient norm: 1.4104612643098329\nDiscriminator gradient norm: 3.1122792936231907\nGenerator gradient norm: 0.47232482469836257\n[Epoch 38/100] [G loss 1.422572] [D loss 0.414697] [G V loss: 1.171809] [D Val Loss: 1.243567]\nDiscriminator gradient norm: 2.968883805503558\nGenerator gradient norm: 0.15545923736957723\nDiscriminator gradient norm: 4.031672323353203\nGenerator gradient norm: 1.6585419602877967\n[Epoch 40/100] [G loss 1.439877] [D loss 0.410334] [G V loss: 0.740098] [D Val Loss: 1.075837]\nDiscriminator gradient norm: 1.9636096057685217\nGenerator gradient norm: 1.1324907461630223\nDiscriminator gradient norm: 2.2450134047037484\nGenerator gradient norm: 1.6745549705624339\n[Epoch 42/100] [G loss 0.974281] [D loss 0.567668] [G V loss: 0.705018] [D Val Loss: 0.629778]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1467903384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_trained_model\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/4249804977.py\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(G, D, pre_trained_model, train_dataloader, val_dataloader, num_epochs, z_dim, n_classes, n_mfcc, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtrain_G_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtrain_D_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelchunks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mothercounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1373756298.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0msample_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mprocessed_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_and_process_waveform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1373756298.py\u001b[0m in \u001b[0;36m_load_and_process_waveform\u001b[0;34m(self, sample_path, target)\u001b[0m\n\u001b[1;32m    110\u001b[0m                                                                        \u001b[0;34m'n_mels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_mel_bins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                                                                        \u001b[0;34m'hop_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                                                                        'win_length': self.frame_length})\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mmfccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmfcc_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/transforms/_transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sample_rate, n_mfcc, dct_type, norm, log_mels, melkwargs)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mmelkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmelkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMelSpectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMelSpectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmelkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mfcc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMelSpectrogram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/transforms/_transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sample_rate, n_fft, win_length, hop_length, f_min, f_max, pad, n_mels, window_fn, power, normalized, wkwargs, center, pad_mode, onesided, norm, mel_scale)\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n\u001b[1;32m    630\u001b[0m         self.mel_scale = MelScale(\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fft\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         )\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/transforms/_transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_mels, sample_rate, f_min, f_max, n_stft, norm, mel_scale)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Require f_min: {} <= f_max: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelscale_fbanks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_stft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mmelscale_fbanks\u001b[0;34m(n_freqs, f_min, f_max, n_mels, sample_rate, norm, mel_scale)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;31m# create filterbank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_triangular_filterbank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_freqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"slaney\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36m_create_triangular_filterbank\u001b[0;34m(all_freqs, f_pts)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;31m# create overlapping triangles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0mzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0mdown_slopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mslopes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mf_diff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (n_freqs, n_filter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0mup_slopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslopes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mf_diff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (n_freqs, n_filter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_slopes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_slopes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def train_w_gan(model, train_loader, criterion, optimizer, device, generated_inputs):\n    model.to(device)\n    train_loss, train_total, train_correct = 0.0, 0, 0\n    model.train()\n    counter = 0\n    for inputchunks, labelchunks in train_loader:\n\n        for x in range(len(inputchunks)):\n            \n            inputs, labels = inputchunks[x].to(device), labelchunks[x].to(device)\n            optimizer.zero_grad()\n            \n            if downsample:\n                inputs = np.squeeze(inputs, axis=1)\n                \n            inputs = inputs.permute(0,2,1)\n            batch_size = inputs.size(0)\n    \n            \n            noise = torch.randn(batch_size, z_dim).to(device)\n            generated_inputs = G(noise, labels)\n            generated_inputs = generated_inputs.permute(0,2,1)\n\n            \n            outputs1 = model(inputs)\n            loss = criterion(outputs1, labels)\n            \n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            train_loss += loss.item()\n            train_total += labels.size(0)\n\n            _, labelacc = labels.max(dim=1)\n            predicted = outputs1.data.argmax(dim=1)\n            train_correct += (predicted == labelacc).sum().item()\n         \n            outputs2 = model(generated_inputs)\n            \n            loss = criterion(outputs2, labels)\n            train_loss += loss.item()\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            counter += (inputs.size(0))\n            train_total += labels.size(0)\n            #_, labels = labels.max(dim=1)\n            _, labelacc = labels.max(dim=1)\n            predicted = outputs2.data.argmax(dim=1)\n\n            train_correct += (predicted == labelacc).sum().item()\n            \n    train_loss /= (len(train_loader.dataset)  * len(inputchunks) * 2)\n    train_acc = train_correct / (train_total)          \n    return train_loss, train_acc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LSTMNet(input_size, hidden_size, num_layers, num_classes)\n\ntrain_losses = []\ntrain_accs = []\nval_losses = []\nval_accs = []\npatience = 5\npatience_counter = 0\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4) # Set weight decay here\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n\nfor epoch in range(num_epochs):\n    # Training\n    train_loss, train_acc = train_w_gan(model, train_loader, criterion, optimizer, device, G)\n\n    # Validation\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n    # Save the model if the validation loss has decreased\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'lstm_net.pth')\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    # Early stopping\n    if patience_counter >= patience:\n        print('Validation loss has not improved for the past', patience, 'epochs. Stopping early.')\n    scheduler.step(val_loss)\n\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\ntest_loss, test_acc = validate(model, test_loader, criterion, device)\n\nprint(\"Test Acc: \", test_acc, \"Test loss: \", test_loss)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-24T19:02:21.441354Z","iopub.execute_input":"2023-04-24T19:02:21.441766Z","iopub.status.idle":"2023-04-24T19:02:21.460089Z","shell.execute_reply.started":"2023-04-24T19:02:21.441728Z","shell.execute_reply":"2023-04-24T19:02:21.459115Z"},"trusted":true},"execution_count":420,"outputs":[]}]}